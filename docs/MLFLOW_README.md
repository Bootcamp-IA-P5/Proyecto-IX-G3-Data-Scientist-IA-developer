# ğŸ“Š MLflow Integration - GuÃ­a Completa

## ğŸ“‹ Tabla de Contenidos

1. [Â¿QuÃ© es MLflow?](#quÃ©-es-mlflow)
2. [Setup e InstalaciÃ³n](#setup-e-instalaciÃ³n)
3. [Estructura del Proyecto](#estructura-del-proyecto)
4. [CÃ³mo Funciona en Este Proyecto](#cÃ³mo-funciona-en-este-proyecto)
5. [CÃ³mo Usar MLflow](#cÃ³mo-usar-mlflow)
6. [Hacer MÃºltiples Experimentos](#hacer-mÃºltiples-experimentos)
7. [Troubleshooting](#troubleshooting)
8. [Referencias](#referencias)

---

## Â¿QuÃ© es MLflow?

**MLflow** es una plataforma open-source para gestionar el ciclo de vida completo de Machine Learning. Te permite:

- âœ… **Tracking de experimentos**: Registrar parÃ¡metros, mÃ©tricas y modelos
- âœ… **Comparar experimentos**: Ver quÃ© configuraciones funcionan mejor
- âœ… **Reproducibilidad**: Guardar exactamente quÃ© cÃ³digo y parÃ¡metros usaste
- âœ… **Versionado de modelos**: Gestionar diferentes versiones de tus modelos
- âœ… **Deployment**: Facilitar el despliegue de modelos en producciÃ³n

### Conceptos Clave

- **Experimento (Experiment)**: Agrupa mÃºltiples runs relacionados (ej: "Random_Forest_Stroke_Prediction")
- **Run**: Una ejecuciÃ³n individual del script (cada vez que entrenas)
- **ParÃ¡metros**: Valores de configuraciÃ³n (n_estimators, max_depth, etc.)
- **MÃ©tricas**: Resultados numÃ©ricos (accuracy, f1_score, etc.)
- **Artifacts**: Archivos guardados (grÃ¡ficos, modelos, CSVs)
- **Tags**: Etiquetas para identificar y filtrar runs

---

## Setup e InstalaciÃ³n

### 1. Instalar MLflow

```bash
pip install mlflow
```

O desde requirements.txt:
```bash
pip install -r requirements.txt
```

### 2. Verificar InstalaciÃ³n

```bash
python -c "import mlflow; print(f'MLflow version: {mlflow.__version__}')"
```

### 3. Configurar .gitignore

AsegÃºrate de que `mlruns/` estÃ¡ en `.gitignore`:

```
# MLflow tracking
mlruns/
```

---

## Estructura del Proyecto

```
proyecto/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ train_random_forest.py    â† Script con MLflow integrado
â”‚   â””â”€â”€ mlruns/                    â† MLflow crea esto automÃ¡ticamente
â”‚       â””â”€â”€ 0/                       â† ID del experimento
â”‚           â””â”€â”€ [hash]/              â† Cada run tiene su carpeta
â”‚               â”œâ”€â”€ metrics/         â† MÃ©tricas registradas
â”‚               â”œâ”€â”€ params/           â† ParÃ¡metros registrados
â”‚               â”œâ”€â”€ artifacts/       â† GrÃ¡ficos, modelos, etc.
â”‚               â””â”€â”€ meta.yaml        â† Metadata del run
â”œâ”€â”€ models/                          â† Modelos para producciÃ³n (pickle)
â”œâ”€â”€ data/                            â† Datos preprocesados (generados por preprocessing)
â””â”€â”€ requirements.txt                 â† Incluye mlflow
```

---

## CÃ³mo Funciona en Este Proyecto

### Script: `notebooks/train_random_forest.py`

El script estÃ¡ completamente integrado con MLflow. AquÃ­ estÃ¡ lo que hace:

#### 1. **ConfiguraciÃ³n del Experimento** (lÃ­nea 48)
```python
mlflow.set_experiment("Random_Forest_Stroke_Prediction")
```
- Crea o selecciona el experimento
- Todos los runs irÃ¡n a este experimento

#### 2. **Inicio del Run** (lÃ­nea 177)
```python
with mlflow.start_run():
    # Todo el cÃ³digo de entrenamiento aquÃ­
```
- Inicia un nuevo run automÃ¡ticamente
- Todo lo que registres va a este run

#### 3. **Tags** (lÃ­neas 182-187)
```python
mlflow.set_tag("model_type", "RandomForest")
mlflow.set_tag("use_smote", "False")
mlflow.set_tag("dataset", "stroke_dataset")
mlflow.set_tag("task", "binary_classification")
mlflow.set_tag("target", "stroke_prediction")
```
- Identifican el tipo de experimento
- Permiten filtrar runs fÃ¡cilmente

#### 4. **Registro de ParÃ¡metros** (lÃ­neas 199-201)
```python
mlflow.log_param("n_estimators", best_params['n_estimators'])
mlflow.log_param("max_depth", best_params['max_depth'])
mlflow.log_param("min_samples_split", best_params['min_samples_split'])
```
- Registra los hiperparÃ¡metros encontrados por Optuna

#### 5. **Registro de MÃ©tricas** (lÃ­neas 266-267, 389-390)
```python
mlflow.log_metric("test_accuracy", test_accuracy)
mlflow.log_metric("test_f1_score", test_f1)
mlflow.log_metric("test_accuracy_optimal", test_accuracy_opt)
mlflow.log_metric("test_f1_score_optimal", test_f1_opt)
```
- Registra mÃ©tricas de evaluaciÃ³n
- Con threshold 0.5 y threshold Ã³ptimo

#### 6. **Guardar Artifacts** (grÃ¡ficos)
```python
mlflow.log_artifact(curves_path, "plots")           # Curvas ROC/PR
mlflow.log_artifact(feature_importance_path, "plots")  # Feature importance
mlflow.log_artifact(feature_importance_csv, "data")    # CSV de features
```
- Guarda grÃ¡ficos y archivos para anÃ¡lisis

#### 7. **Guardar Modelo** (lÃ­neas 508-513)
```python
mlflow.sklearn.log_model(
    rf_model,
    "model",
    registered_model_name="RandomForest_Stroke_Prediction"
)
```
- Guarda el modelo entrenado en MLflow
- Permite versionado y carga posterior

---

## CÃ³mo Usar MLflow

### Paso 1: Preparar los Datos

**IMPORTANTE**: Antes de ejecutar el script, necesitas los datos preprocesados.

Ejecuta el notebook de preprocessing:
```bash
# Abre notebooks/stroke_preprocessing.ipynb
# Ejecuta todas las celdas
# Esto generarÃ¡ los archivos .pkl en data/
```

Los archivos necesarios:
- `data/X_train_balanced.pkl`
- `data/y_train_balanced.pkl`
- `data/X_val_scaled.pkl`
- `data/y_val.pkl`
- `data/X_test_scaled.pkl`
- `data/y_test.pkl`

### Paso 2: Ejecutar el Script

```bash
cd notebooks
python train_random_forest.py
```

**QuÃ© verÃ¡s:**
```
================================================================================
ğŸŒ² RANDOM FOREST - PREDICCIÃ“N DE ICTUS
================================================================================
ğŸ“Š MLflow experiment: Random_Forest_Stroke_Prediction
ğŸ“‚ CARGA DE DATOS
âœ… Datos cargados desde: ../data
...
ğŸ“Š MLFLOW: Run iniciado
...
âœ… Curvas guardadas en MLflow como artifact
âœ… Feature importance guardada en MLflow como artifact
âœ… Modelo guardado en MLflow
ğŸ“Š MLFLOW: Run completado y registrado
```

### Paso 3: Abrir MLflow UI

En una **nueva terminal**, desde la raÃ­z del proyecto:

```bash
mlflow ui
```

VerÃ¡s:
```
[INFO] Starting gunicorn 20.1.0
[INFO] Listening at: http://127.0.0.1:5000
```

Abre en tu navegador: **http://localhost:5000**

### Paso 4: Explorar en MLflow UI

#### PÃ¡gina Principal
- Lista de experimentos
- Click en "Random_Forest_Stroke_Prediction"

#### Vista del Experimento
- Tabla con todos tus runs
- Columnas: fecha, parÃ¡metros, mÃ©tricas
- Ordena por cualquier columna (click en el header)

#### Detalles de un Run
- Click en cualquier run para ver:
  - **ParÃ¡metros**: Todos los hiperparÃ¡metros usados
  - **MÃ©tricas**: Todas las mÃ©tricas registradas
  - **Tags**: Etiquetas del run
  - **Artifacts**: 
    - `plots/random_forest_curves.png` - GrÃ¡ficos ROC/PR
    - `plots/feature_importance_rf.png` - Importancia de features
    - `data/feature_importance.csv` - CSV con importancia
    - `model/` - Modelo completo (descargable)

#### Comparar Runs
1. Selecciona 2+ runs (checkboxes)
2. Click en "Compare"
3. VerÃ¡s comparaciÃ³n lado a lado
4. GrÃ¡ficos comparando mÃ©tricas

---

## Hacer MÃºltiples Experimentos

### Ejemplo: Probar diferentes n_estimators

#### MÃ©todo 1: Modificar el Script Manualmente

**Para probar n_estimators = 50:**

1. Modifica lÃ­nea 177:
```python
with mlflow.start_run(run_name="n_estimators_50"):
```

2. Modifica lÃ­nea 115 (en la funciÃ³n objective):
```python
'n_estimators': 50,  # Fijar en 50
# Comenta: 'n_estimators': trial.suggest_int('n_estimators', 50, 300),
```

3. Ejecuta:
```bash
python train_random_forest.py
```

4. Repite para otros valores (100, 200) cambiando el nombre y el valor

#### MÃ©todo 2: Usar un Loop (Avanzado)

Puedes modificar el script para hacer mÃºltiples runs automÃ¡ticamente:

```python
n_estimators_values = [50, 100, 200]

for n_est in n_estimators_values:
    with mlflow.start_run(run_name=f"n_estimators_{n_est}"):
        mlflow.set_tag("n_estimators_test", str(n_est))
        # ... resto del cÃ³digo con n_estimators fijado en n_est
```

### Comparar Resultados

1. Abre MLflow UI
2. Ve a tu experimento
3. Selecciona los 3 runs (n_estimators_50, 100, 200)
4. Click en "Compare"
5. Ordena por `test_f1_score` para ver cuÃ¡l es mejor

---

## QuÃ© se Registra en MLflow

### ParÃ¡metros (3)
- `n_estimators`: NÃºmero de Ã¡rboles
- `max_depth`: Profundidad mÃ¡xima
- `min_samples_split`: MÃ­nimo de muestras para split

### MÃ©tricas (4)
- `test_accuracy`: Accuracy en test (threshold 0.5)
- `test_f1_score`: F1-Score en test (threshold 0.5)
- `test_accuracy_optimal`: Accuracy con threshold Ã³ptimo
- `test_f1_score_optimal`: F1-Score con threshold Ã³ptimo

### Tags (5)
- `model_type`: "RandomForest"
- `use_smote`: "False"
- `dataset`: "stroke_dataset"
- `task`: "binary_classification"
- `target`: "stroke_prediction"

### Artifacts (4)
- `plots/random_forest_curves.png` - Curvas ROC y Precision-Recall
- `plots/feature_importance_rf.png` - GrÃ¡fico de importancia
- `data/feature_importance.csv` - CSV con importancia de features
- `model/` - Modelo entrenado completo

---

## Troubleshooting

### Error: "No module named 'mlflow'"

**SoluciÃ³n:**
```bash
pip install mlflow
```

### Error: "No se encontrÃ³ la carpeta de datos preprocesados"

**SoluciÃ³n:**
1. Ejecuta `notebooks/stroke_preprocessing.ipynb`
2. O verifica que los archivos `.pkl` estÃ©n en `data/` o `backend/data/`

### Error: "mlflow ui: command not found"

**SoluciÃ³n:**
```bash
# AsegÃºrate de que MLflow estÃ¡ instalado
pip install mlflow

# O usa:
python -m mlflow ui
```

### No veo mi experimento en MLflow UI

**SoluciÃ³n:**
1. Verifica que el script terminÃ³ correctamente
2. Refresca la pÃ¡gina (F5)
3. Verifica que estÃ¡s en la carpeta correcta:
```bash
ls mlruns/  # Debe mostrar carpetas con nÃºmeros
```

### Los artifacts no aparecen

**SoluciÃ³n:**
1. Verifica que los grÃ¡ficos se generaron correctamente
2. Revisa los paths en el script
3. Verifica permisos de escritura

### MLflow UI no se abre

**SoluciÃ³n:**
1. Verifica que el puerto 5000 no estÃ¡ en uso:
```bash
lsof -i :5000
```
2. Usa otro puerto:
```bash
mlflow ui --port 5001
```

---

## Comandos Ãštiles

### Ver experimentos desde terminal
```bash
mlflow experiments list
```

### Ver runs de un experimento
```bash
mlflow runs list --experiment-id 0
```

### Cargar modelo desde MLflow
```python
import mlflow
model = mlflow.sklearn.load_model("runs:/<run_id>/model")
```

### Exportar datos de MLflow
```bash
# Exportar a CSV
mlflow export-metrics --experiment-id 0 --output-file metrics.csv
```

---

## Estructura de mlruns/

```
mlruns/
â””â”€â”€ 0/                          â† ID del experimento
    â”œâ”€â”€ meta.yaml               â† Metadata del experimento
    â””â”€â”€ [hash-del-run]/         â† Cada run tiene un hash Ãºnico
        â”œâ”€â”€ metrics/
        â”‚   â”œâ”€â”€ test_accuracy
        â”‚   â”œâ”€â”€ test_f1_score
        â”‚   â””â”€â”€ ...
        â”œâ”€â”€ params/
        â”‚   â”œâ”€â”€ n_estimators
        â”‚   â”œâ”€â”€ max_depth
        â”‚   â””â”€â”€ ...
        â”œâ”€â”€ tags/
        â”‚   â”œâ”€â”€ model_type
        â”‚   â”œâ”€â”€ use_smote
        â”‚   â””â”€â”€ ...
        â”œâ”€â”€ artifacts/
        â”‚   â”œâ”€â”€ plots/
        â”‚   â”‚   â”œâ”€â”€ random_forest_curves.png
        â”‚   â”‚   â””â”€â”€ feature_importance_rf.png
        â”‚   â”œâ”€â”€ data/
        â”‚   â”‚   â””â”€â”€ feature_importance.csv
        â”‚   â””â”€â”€ model/
        â”‚       â””â”€â”€ [archivos del modelo]
        â””â”€â”€ meta.yaml            â† Metadata del run
```

---

## Mejores PrÃ¡cticas

### 1. Nombres de Runs
- Usa nombres descriptivos: `"n_estimators_50"`, `"with_smote_v1"`
- Evita nombres genÃ©ricos como `"run1"`, `"test"`

### 2. Tags
- Usa tags consistentes para poder filtrar
- Ejemplo: siempre usa `use_smote: "True"` o `"False"` (no mezcles True/true)

### 3. MÃ©tricas
- Registra mÃ©tricas en el mismo conjunto de datos (test)
- Usa nombres consistentes: `test_accuracy`, no `accuracy_test`

### 4. Artifacts
- Organiza artifacts en carpetas: `plots/`, `data/`, `models/`
- No guardes archivos muy grandes (mejor comprimir)

### 5. Experimentos
- Un experimento por tipo de modelo
- Ejemplo: "Random_Forest_Stroke", "XGBoost_Stroke", etc.

---

## PrÃ³ximos Pasos

### Mejoras Futuras

1. **Tracking Remoto**
   - Configurar servidor MLflow
   - Backend store (PostgreSQL)
   - Artifact store (S3, Azure Blob)

2. **Autologging**
   - Usar `mlflow.sklearn.autolog()` para registro automÃ¡tico

3. **Model Registry**
   - Registrar modelos para producciÃ³n
   - GestiÃ³n de versiones

4. **IntegraciÃ³n CI/CD**
   - Registrar experimentos automÃ¡ticamente
   - Comparar modelos en cada commit

---

## Referencias

- [DocumentaciÃ³n oficial de MLflow](https://www.mlflow.org/docs/latest/index.html)
- [MLflow Tracking](https://www.mlflow.org/docs/latest/tracking.html)
- [MLflow Models](https://www.mlflow.org/docs/latest/models.html)

---

## Resumen RÃ¡pido

```bash
# 1. Instalar
pip install mlflow

# 2. Generar datos (si no existen)
# Ejecutar notebooks/stroke_preprocessing.ipynb

# 3. Entrenar modelo
cd notebooks
python train_random_forest.py

# 4. Ver resultados
cd ..
mlflow ui
# Abrir http://localhost:5000
```

---

## Script de VerificaciÃ³n

Antes de ejecutar el entrenamiento, puedes verificar que todo estÃ© listo:

```bash
cd notebooks
python verify_mlflow_setup.py
```

Este script verifica:
- âœ… Todas las librerÃ­as instaladas
- âœ… Datos preprocesados disponibles
- âœ… Estructura de carpetas correcta
- âœ… MLflow funcionando
- âœ… Script de entrenamiento con MLflow integrado

---

## Checklist de ImplementaciÃ³n

### âœ… Completado

- [x] MLflow instalado en requirements.txt
- [x] mlruns/ en .gitignore
- [x] Script train_random_forest.py con MLflow integrado
- [x] Registro de parÃ¡metros (3)
- [x] Registro de mÃ©tricas (4)
- [x] Guardado de artifacts (grÃ¡ficos y CSV)
- [x] Guardado de modelo en MLflow
- [x] Tags para identificar runs
- [x] Script de verificaciÃ³n
- [x] README completo

### â³ Pendiente (depende de ti)

- [ ] Ejecutar stroke_preprocessing.ipynb para generar datos
- [ ] Ejecutar train_random_forest.py
- [ ] Abrir MLflow UI y verificar resultados

---

**Â¿Preguntas?** Revisa la secciÃ³n de Troubleshooting o consulta la documentaciÃ³n oficial de MLflow.


