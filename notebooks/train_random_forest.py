"""
Script para entrenar Random Forest con Optuna y K-Folds
"""

import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')  # Para guardar gr√°ficos sin display
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import warnings
import os
import sys
warnings.filterwarnings('ignore')

# Machine Learning
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report,
    roc_curve, precision_recall_curve
)

# Hyperparameter Optimization
import optuna
from optuna.visualization import plot_optimization_history, plot_param_importances

# MLflow Tracking
import mlflow
import mlflow.sklearn

# Configuraci√≥n
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("="*80)
print("üå≤ RANDOM FOREST - PREDICCI√ìN DE ICTUS")
print("="*80)
print("\n‚úÖ Librer√≠as importadas correctamente")
print(f"üìä Optuna version: {optuna.__version__}")
print(f"üî¢ Scikit-learn version: {__import__('sklearn').__version__}")

# ============================================================================
# CONFIGURAR MLFLOW
# ============================================================================
mlflow.set_experiment("Random_Forest_Stroke_Prediction")
print(f"üìä MLflow experiment: Random_Forest_Stroke_Prediction")

# ============================================================================
# CARGA DE DATOS
# ============================================================================
print("\n" + "="*80)
print("üìÇ CARGA DE DATOS")
print("="*80)

try:
    with open('../data/X_train_balanced.pkl', 'rb') as f:
        X_train_balanced = pickle.load(f)
    with open('../data/y_train_balanced.pkl', 'rb') as f:
        y_train_balanced = pickle.load(f)
    with open('../data/X_val_scaled.pkl', 'rb') as f:
        X_val_scaled = pickle.load(f)
    with open('../data/y_val.pkl', 'rb') as f:
        y_val = pickle.load(f)
    with open('../data/X_test_scaled.pkl', 'rb') as f:
        X_test_scaled = pickle.load(f)
    with open('../data/y_test.pkl', 'rb') as f:
        y_test = pickle.load(f)
    print("‚úÖ Datos cargados desde archivos pickle")
except FileNotFoundError as e:
    print(f"‚ùå Error: {e}")
    sys.exit(1)

print("\nüìä RESUMEN DE DATOS:")
print(f"   Train: {X_train_balanced.shape[0]:,} muestras, {X_train_balanced.shape[1]} features")
print(f"   Validation: {X_val_scaled.shape[0]:,} muestras, {X_val_scaled.shape[1]} features")
print(f"   Test: {X_test_scaled.shape[0]:,} muestras, {X_test_scaled.shape[1]} features")
print(f"\n   Train balanceado - Stroke 0: {(y_train_balanced == 0).sum():,}, Stroke 1: {(y_train_balanced == 1).sum():,}")
print(f"   Validation original - Stroke 0: {(y_val == 0).sum():,}, Stroke 1: {(y_val == 1).sum():,}")
print(f"   Test original - Stroke 0: {(y_test == 0).sum():,}, Stroke 1: {(y_test == 1).sum():,}")

# ============================================================================
# CONFIGURACI√ìN
# ============================================================================
print("\n" + "="*80)
print("‚öôÔ∏è CONFIGURACI√ìN")
print("="*80)

N_FOLDS = 5
RANDOM_STATE = 42
skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)

print(f"‚úÖ Configuraci√≥n:")
print(f"   ‚Ä¢ K-Folds: {N_FOLDS}")
print(f"   ‚Ä¢ Random State: {RANDOM_STATE}")
print(f"   ‚Ä¢ M√©trica objetivo: F1-Score (balance entre Precision y Recall)")
print(f"   ‚Ä¢ M√©tricas adicionales: Recall, AUC-ROC")

# ============================================================================
# FUNCI√ìN OBJETIVO PARA OPTUNA
# ============================================================================
print("\n" + "="*80)
print("üéØ FUNCI√ìN OBJETIVO PARA OPTUNA")
print("="*80)

def objective(trial):
    """
    Funci√≥n objetivo para Optuna.
    Optimiza hiperpar√°metros de Random Forest usando K-Folds CV.
    """
    # Hiperpar√°metros a optimizar
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
        'max_depth': trial.suggest_int('max_depth', 5, 30),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),
        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),
        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),
        'class_weight': 'balanced',  # Balanceo autom√°tico de clases
        'random_state': RANDOM_STATE,
        'n_jobs': -1  # Usar todos los cores
    }
    
    # Crear modelo
    model = RandomForestClassifier(**params)
    
    # K-Folds Cross-Validation con F1-Score
    cv_scores = cross_val_score(
        model, 
        X_train_balanced, 
        y_train_balanced,
        cv=skf,
        scoring='f1',
        n_jobs=-1
    )
    
    # Retornar el promedio de F1-Score
    return cv_scores.mean()

print("‚úÖ Funci√≥n objetivo definida")
print("   Optimizando: n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features, bootstrap")

# ============================================================================
# OPTIMIZACI√ìN CON OPTUNA
# ============================================================================
print("\n" + "="*80)
print("üöÄ OPTIMIZACI√ìN CON OPTUNA")
print("="*80)
print(f"üìä Datos: {X_train_balanced.shape[0]:,} muestras, {X_train_balanced.shape[1]} features")
print(f"üîÑ K-Folds: {N_FOLDS}")
print(f"‚è±Ô∏è  Esto puede tomar varios minutos...\n")

# Crear estudio de Optuna
study = optuna.create_study(
    direction='maximize',  # Maximizar F1-Score
    study_name='random_forest_stroke_prediction',
    sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE)
)

# Ejecutar optimizaci√≥n
N_TRIALS = 50  # N√∫mero de trials
study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)

print("\n" + "="*80)
print("‚úÖ OPTIMIZACI√ìN COMPLETADA")
print("="*80)
print(f"\nüèÜ MEJOR F1-SCORE: {study.best_value:.4f}")
print(f"\nüìã MEJORES HIPERPAR√ÅMETROS:")
for key, value in study.best_params.items():
    print(f"   ‚Ä¢ {key}: {value}")

# ============================================================================
# INICIAR MLFLOW RUN
# ============================================================================
with mlflow.start_run():
    print("\n" + "="*80)
    print("üìä MLFLOW: Run iniciado")
    print("="*80)

    # ============================================================================
    # ENTRENAR MODELO FINAL
    # ============================================================================
    print("\n" + "="*80)
    print("üéì ENTRENAR MODELO FINAL")
    print("="*80)

    # Obtener mejores par√°metros
    best_params = study.best_params.copy()
    best_params['class_weight'] = 'balanced'
    best_params['random_state'] = RANDOM_STATE
    best_params['n_jobs'] = -1

    print("üéì ENTRENANDO MODELO FINAL...")
    print(f"üìã Par√°metros: {best_params}\n")
    
    # Registrar par√°metros en MLflow
    mlflow.log_param("n_estimators", best_params['n_estimators'])
    mlflow.log_param("max_depth", best_params['max_depth'])
    mlflow.log_param("min_samples_split", best_params['min_samples_split'])

    # Crear y entrenar modelo
    rf_model = RandomForestClassifier(**best_params)
    rf_model.fit(X_train_balanced, y_train_balanced)

    print("‚úÖ Modelo entrenado correctamente")
    print(f"   ‚Ä¢ N√∫mero de √°rboles: {rf_model.n_estimators}")
    print(f"   ‚Ä¢ Features importantes: {rf_model.n_features_in_}")

    # ============================================================================
    # EVALUACI√ìN EN VALIDATION SET
    # ============================================================================
    print("\n" + "="*80)
    print("üìä EVALUACI√ìN EN VALIDATION SET")
    print("="*80)

    # Predicciones
    y_val_pred = rf_model.predict(X_val_scaled)
    y_val_pred_proba = rf_model.predict_proba(X_val_scaled)[:, 1]

    # M√©tricas
    val_accuracy = accuracy_score(y_val, y_val_pred)
    val_precision = precision_score(y_val, y_val_pred)
    val_recall = recall_score(y_val, y_val_pred)
    val_f1 = f1_score(y_val, y_val_pred)
    val_auc = roc_auc_score(y_val, y_val_pred_proba)

    print(f"\nüìä M√âTRICAS:")
    print(f"   Accuracy:  {val_accuracy:.4f}")
    print(f"   Precision: {val_precision:.4f}")
    print(f"   Recall:    {val_recall:.4f} ‚≠ê (M√âTRICA PRINCIPAL)")
    print(f"   F1-Score:  {val_f1:.4f} ‚≠ê")
    print(f"   AUC-ROC:   {val_auc:.4f} ‚≠ê")

    print(f"\nüìã MATRIZ DE CONFUSI√ìN:")
    cm_val = confusion_matrix(y_val, y_val_pred)
    print(cm_val)
    print(f"\n   Verdaderos Negativos: {cm_val[0,0]}")
    print(f"   Falsos Positivos:     {cm_val[0,1]}")
    print(f"   Falsos Negativos:     {cm_val[1,0]} ‚ö†Ô∏è  (CR√çTICO - pacientes en riesgo no detectados)")
    print(f"   Verdaderos Positivos: {cm_val[1,1]}")

    print(f"\nüìÑ REPORTE DE CLASIFICACI√ìN:")
    print(classification_report(y_val, y_val_pred, target_names=['No Stroke', 'Stroke']))

    # ============================================================================
    # EVALUACI√ìN EN TEST SET
    # ============================================================================
    print("\n" + "="*80)
    print("üß™ EVALUACI√ìN EN TEST SET (DATOS FINALES)")
    print("="*80)

    # Predicciones
    y_test_pred = rf_model.predict(X_test_scaled)
    y_test_pred_proba = rf_model.predict_proba(X_test_scaled)[:, 1]

    # M√©tricas
    test_accuracy = accuracy_score(y_test, y_test_pred)
    test_precision = precision_score(y_test, y_test_pred)
    test_recall = recall_score(y_test, y_test_pred)
    test_f1 = f1_score(y_test, y_test_pred)
    test_auc = roc_auc_score(y_test, y_test_pred_proba)
    
    # Registrar m√©tricas de test (threshold 0.5) en MLflow
    mlflow.log_metric("test_accuracy", test_accuracy)
    mlflow.log_metric("test_f1_score", test_f1)

    print(f"\nüìä M√âTRICAS FINALES:")
    print(f"   Accuracy:  {test_accuracy:.4f}")
    print(f"   Precision: {test_precision:.4f}")
    print(f"   Recall:    {test_recall:.4f} ‚≠ê (M√âTRICA PRINCIPAL)")
    print(f"   F1-Score:  {test_f1:.4f} ‚≠ê")
    print(f"   AUC-ROC:   {test_auc:.4f} ‚≠ê")

    print(f"\nüìã MATRIZ DE CONFUSI√ìN:")
    cm_test = confusion_matrix(y_test, y_test_pred)
    print(cm_test)
    print(f"\n   Verdaderos Negativos: {cm_test[0,0]}")
    print(f"   Falsos Positivos:     {cm_test[0,1]}")
    print(f"   Falsos Negativos:     {cm_test[1,0]} ‚ö†Ô∏è  (CR√çTICO)")
    print(f"   Verdaderos Positivos: {cm_test[1,1]}")

    print(f"\nüìÑ REPORTE DE CLASIFICACI√ìN:")
    print(classification_report(y_test, y_test_pred, target_names=['No Stroke', 'Stroke']))

    # ============================================================================
    # AJUSTE DE THRESHOLD √ìPTIMO
    # ============================================================================
    print("\n" + "="*80)
    print("üéØ AJUSTE DE THRESHOLD √ìPTIMO")
    print("="*80)
    print("Buscando threshold que maximice Recall (m√≠nimo 0.70) manteniendo F1 razonable...\n")

    # Probar diferentes thresholds en validation set
    thresholds = np.arange(0.1, 0.6, 0.05)
    best_threshold = 0.5
    best_recall = 0
    best_f1 = 0
    best_metrics = None

    results_threshold = []

    for threshold in thresholds:
        # Predicciones con threshold ajustado
        y_val_pred_thresh = (y_val_pred_proba >= threshold).astype(int)
        
        # Calcular m√©tricas
        recall_thresh = recall_score(y_val, y_val_pred_thresh)
        precision_thresh = precision_score(y_val, y_val_pred_thresh)
        f1_thresh = f1_score(y_val, y_val_pred_thresh)
        
        results_threshold.append({
            'threshold': threshold,
            'recall': recall_thresh,
            'precision': precision_thresh,
            'f1': f1_thresh
        })
        
        # Buscar threshold que maximice Recall (objetivo: >0.70) con F1 razonable (>0.40)
        if recall_thresh >= 0.70 and f1_thresh > best_f1:
            best_threshold = threshold
            best_recall = recall_thresh
            best_f1 = f1_thresh
            best_metrics = {
                'recall': recall_thresh,
                'precision': precision_thresh,
                'f1': f1_thresh
            }

    # Si no encontramos threshold con Recall >0.70, usar el que maximice F1 con Recall >0.50
    if best_recall < 0.70:
        print("‚ö†Ô∏è  No se encontr√≥ threshold con Recall >0.70. Buscando mejor compromiso...\n")
        for result in results_threshold:
            if result['recall'] >= 0.50 and result['f1'] > best_f1:
                best_threshold = result['threshold']
                best_recall = result['recall']
                best_f1 = result['f1']
                best_metrics = {
                    'recall': result['recall'],
                    'precision': result['precision'],
                    'f1': result['f1']
                }

    print(f"‚úÖ THRESHOLD √ìPTIMO ENCONTRADO: {best_threshold:.3f}")
    print(f"   Validation - Recall: {best_metrics['recall']:.4f}, Precision: {best_metrics['precision']:.4f}, F1: {best_metrics['f1']:.4f}")

    # Tabla de resultados
    print(f"\nüìä RESULTADOS POR THRESHOLD (Top 10 mejores F1):")
    df_threshold = pd.DataFrame(results_threshold).sort_values('f1', ascending=False)
    print(df_threshold.head(10).to_string(index=False))

    # ============================================================================
    # RE-EVALUACI√ìN CON THRESHOLD √ìPTIMO
    # ============================================================================
    print("\n" + "="*80)
    print("üîÑ RE-EVALUACI√ìN CON THRESHOLD √ìPTIMO")
    print("="*80)

    # Validation con threshold √≥ptimo
    y_val_pred_optimal = (y_val_pred_proba >= best_threshold).astype(int)
    val_recall_opt = recall_score(y_val, y_val_pred_optimal)
    val_precision_opt = precision_score(y_val, y_val_pred_optimal)
    val_f1_opt = f1_score(y_val, y_val_pred_optimal)
    val_accuracy_opt = accuracy_score(y_val, y_val_pred_optimal)

    print(f"\nüìä VALIDATION SET (Threshold = {best_threshold:.3f}):")
    print(f"   Accuracy:  {val_accuracy_opt:.4f}")
    print(f"   Precision: {val_precision_opt:.4f}")
    print(f"   Recall:    {val_recall_opt:.4f} ‚≠ê (M√âTRICA PRINCIPAL)")
    print(f"   F1-Score:  {val_f1_opt:.4f} ‚≠ê")

    cm_val_opt = confusion_matrix(y_val, y_val_pred_optimal)
    print(f"\nüìã MATRIZ DE CONFUSI√ìN:")
    print(cm_val_opt)
    print(f"\n   Verdaderos Negativos: {cm_val_opt[0,0]}")
    print(f"   Falsos Positivos:     {cm_val_opt[0,1]}")
    print(f"   Falsos Negativos:     {cm_val_opt[1,0]} ‚ö†Ô∏è  (CR√çTICO)")
    print(f"   Verdaderos Positivos: {cm_val_opt[1,1]}")

    # Test con threshold √≥ptimo
    y_test_pred_optimal = (y_test_pred_proba >= best_threshold).astype(int)
    test_recall_opt = recall_score(y_test, y_test_pred_optimal)
    test_precision_opt = precision_score(y_test, y_test_pred_optimal)
    test_f1_opt = f1_score(y_test, y_test_pred_optimal)
    test_accuracy_opt = accuracy_score(y_test, y_test_pred_optimal)
    
    # Registrar m√©tricas de test con threshold √≥ptimo en MLflow
    mlflow.log_metric("test_accuracy_optimal", test_accuracy_opt)
    mlflow.log_metric("test_f1_score_optimal", test_f1_opt)

    print(f"\nüìä TEST SET (Threshold = {best_threshold:.3f}):")
    print(f"   Accuracy:  {test_accuracy_opt:.4f}")
    print(f"   Precision: {test_precision_opt:.4f}")
    print(f"   Recall:    {test_recall_opt:.4f} ‚≠ê (M√âTRICA PRINCIPAL)")
    print(f"   F1-Score:  {test_f1_opt:.4f} ‚≠ê")

    cm_test_opt = confusion_matrix(y_test, y_test_pred_optimal)
    print(f"\nüìã MATRIZ DE CONFUSI√ìN:")
    print(cm_test_opt)
    print(f"\n   Verdaderos Negativos: {cm_test_opt[0,0]}")
    print(f"   Falsos Positivos:     {cm_test_opt[0,1]}")
    print(f"   Falsos Negativos:     {cm_test_opt[1,0]} ‚ö†Ô∏è  (CR√çTICO)")
    print(f"   Verdaderos Positivos: {cm_test_opt[1,1]}")

    print(f"\nüìÑ REPORTE DE CLASIFICACI√ìN:")
    print(classification_report(y_test, y_test_pred_optimal, target_names=['No Stroke', 'Stroke']))

    # ============================================================================
    # VISUALIZACIONES
    # ============================================================================
    print("\n" + "="*80)
    print("üìä GENERANDO VISUALIZACIONES")
    print("="*80)

    # Curvas ROC y Precision-Recall
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))

    # 1. CURVA ROC
    fpr_val, tpr_val, _ = roc_curve(y_val, y_val_pred_proba)
    fpr_test, tpr_test, _ = roc_curve(y_test, y_test_pred_proba)

    axes[0].plot(fpr_val, tpr_val, label=f'Validation (AUC = {val_auc:.3f})', linewidth=2)
    axes[0].plot(fpr_test, tpr_test, label=f'Test (AUC = {test_auc:.3f})', linewidth=2)
    axes[0].plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)
    axes[0].set_xlabel('False Positive Rate', fontsize=12)
    axes[0].set_ylabel('True Positive Rate', fontsize=12)
    axes[0].set_title('ROC Curve - Random Forest', fontsize=14, fontweight='bold')
    axes[0].legend(fontsize=11)
    axes[0].grid(True, alpha=0.3)

    # 2. CURVA PRECISION-RECALL
    precision_val, recall_val, _ = precision_recall_curve(y_val, y_val_pred_proba)
    precision_test, recall_test, _ = precision_recall_curve(y_test, y_test_pred_proba)

    axes[1].plot(recall_val, precision_val, label=f'Validation (F1 = {val_f1:.3f})', linewidth=2)
    axes[1].plot(recall_test, precision_test, label=f'Test (F1 = {test_f1:.3f})', linewidth=2)
    axes[1].set_xlabel('Recall', fontsize=12)
    axes[1].set_ylabel('Precision', fontsize=12)
    axes[1].set_title('Precision-Recall Curve - Random Forest', fontsize=14, fontweight='bold')
    axes[1].legend(fontsize=11)
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    os.makedirs('../', exist_ok=True)
    plt.savefig('../random_forest_curves.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úÖ Curvas guardadas: backend/random_forest_curves.png")

    # Importancia de Features
    feature_importance = pd.DataFrame({
        'feature': X_train_balanced.columns,
        'importance': rf_model.feature_importances_
    }).sort_values('importance', ascending=False)

    print("\n" + "="*80)
    print("üå≥ TOP 15 FEATURES M√ÅS IMPORTANTES")
    print("="*80)
    print(feature_importance.head(15).to_string(index=False))

    # Visualizaci√≥n
    plt.figure(figsize=(12, 8))
    top_features = feature_importance.head(15)
    plt.barh(range(len(top_features)), top_features['importance'], color='steelblue')
    plt.yticks(range(len(top_features)), top_features['feature'])
    plt.xlabel('Importancia', fontsize=12)
    plt.title('Top 15 Features m√°s Importantes - Random Forest', fontsize=14, fontweight='bold')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.savefig('../feature_importance_rf.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("\n‚úÖ Visualizaci√≥n guardada: backend/feature_importance_rf.png")

    # ============================================================================
    # GUARDAR MODELO
    # ============================================================================
    print("\n" + "="*80)
    print("üíæ GUARDANDO MODELO Y RESULTADOS")
    print("="*80)

    import joblib

    # Crear carpeta models si no existe
    os.makedirs('../models', exist_ok=True)

    # Guardar modelo
    joblib.dump(rf_model, '../models/random_forest_model.pkl')
    print("‚úÖ Modelo guardado: models/random_forest_model.pkl")

    # Guardar mejores par√°metros
    with open('../models/rf_best_params.pkl', 'wb') as f:
        pickle.dump(best_params, f)
    print("‚úÖ Mejores par√°metros guardados: models/rf_best_params.pkl")

    # Guardar resultados de evaluaci√≥n
    results = {
        'validation_threshold_0.5': {
            'accuracy': val_accuracy,
            'precision': val_precision,
            'recall': val_recall,
            'f1_score': val_f1,
            'auc_roc': val_auc
        },
        'test_threshold_0.5': {
            'accuracy': test_accuracy,
            'precision': test_precision,
            'recall': test_recall,
            'f1_score': test_f1,
            'auc_roc': test_auc
        },
        'validation_threshold_optimal': {
            'threshold': float(best_threshold),
            'accuracy': val_accuracy_opt,
            'precision': val_precision_opt,
            'recall': val_recall_opt,
            'f1_score': val_f1_opt,
            'auc_roc': val_auc
        },
        'test_threshold_optimal': {
            'threshold': float(best_threshold),
            'accuracy': test_accuracy_opt,
            'precision': test_precision_opt,
            'recall': test_recall_opt,
            'f1_score': test_f1_opt,
            'auc_roc': test_auc
        },
        'best_params': best_params,
        'optimal_threshold': float(best_threshold),
        'feature_importance': feature_importance.to_dict('records')
    }

    with open('../models/rf_results.pkl', 'wb') as f:
        pickle.dump(results, f)
    print("‚úÖ Resultados guardados: models/rf_results.pkl")

    print("\n" + "="*80)
    print("üéâ ENTRENAMIENTO COMPLETADO")
    print("="*80)
    print(f"\nüìä RESUMEN FINAL (Threshold = 0.5):")
    print(f"   Validation - Recall: {val_recall:.4f}, F1: {val_f1:.4f}, AUC: {val_auc:.4f}")
    print(f"   Test       - Recall: {test_recall:.4f}, F1: {test_f1:.4f}, AUC: {test_auc:.4f}")

    print(f"\nüìä RESUMEN FINAL (Threshold √ìptimo = {best_threshold:.3f}):")
    print(f"   Validation - Recall: {val_recall_opt:.4f}, F1: {val_f1_opt:.4f}, AUC: {val_auc:.4f}")
    print(f"   Test       - Recall: {test_recall_opt:.4f}, F1: {test_f1_opt:.4f}, AUC: {test_auc:.4f}")

    print(f"\n‚úÖ Modelo listo para usar en producci√≥n")
    print(f"‚úÖ Threshold √≥ptimo recomendado: {best_threshold:.3f}")
    print("="*80)
    
    print("\nüìä MLFLOW: Run completado y registrado")
    print("="*80)

